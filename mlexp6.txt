

## 6 Wholesale Customers Unsupervised Learning ##

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from sklearn.preprocessing import normalize
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
%matplotlib inline
     
data=pd.read_csv("Wholesale customers data.csv")
print(data.head())

#normalize the data
scaled=normalize(data)
scaled=pd.DataFrame(scaled,columns=data.columns)
print(scaled.head())

#dendrogram to determine the number of clusters
#x axis: samples ; y axis: distance between samples
plt.figure(figsize=(10,7))
plt.title("Dendrogram")
Z=linkage(scaled,method='ward')
dendrograms=dendrogram(Z)

#threshold
#from the dendrogram we choose y=6 as the threshold
plt.figure(figsize=(10,7))
plt.title("Dendrogram")
Z=linkage(scaled,method='ward')
dendrograms=dendrogram(Z)
plt.axhline(y=6,color='black')

#the line cuts at two points hence we have 2 clusters
cluster=AgglomerativeClustering(n_clusters=2,linkage='ward')
cluster.fit_predict(scaled)

#visualization
plt.figure(figsize=(10,7))
plt.scatter(scaled['Milk'],scaled['Grocery'],c=cluster.labels_,cmap = mcolors.ListedColormap(["yellow", "green"]))

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
# Load the dataset
file_path = '/content/Wholesale customers data.csv'
wholesale_data = pd.read_csv(file_path)
# Select the spending columns for clustering
spending_data = wholesale_data.iloc[:, 2:]
# Normalize the data
scaler = StandardScaler()
spending_data_normalized = scaler.fit_transform(spending_data)
# Plot the dendrogram to find the optimal number of clusters
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(spending_data_normalized, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean Distances')
plt.show()
# Apply Agglomerative Clustering with 4 clusters, remove affinity when linkage is 'ward'
agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')
cluster_labels = agg_clustering.fit_predict(spending_data_normalized)
# Calculate the silhouette score to evaluate clustering
silhouette_avg = silhouette_score(spending_data_normalized, cluster_labels)
# Output the silhouette score
silhouette_avg

-------------------------------------------------------------------------------------

## Adult Income Dimensionality Reduction ##

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
url = "adult_dataset.csv"
columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',
           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',
           'hours-per-week', 'native-country', 'income']
data = pd.read_csv(url, names=columns, na_values=' ?')
     
data.dropna(inplace=True)

label_encoders = {}
for column in data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    data[column] = label_encoders[column].fit_transform(data[column])

X = data.drop('income', axis=1)
y = data['income']
     
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
     
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
     
accuracy_scores = []
     
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)
     
accuracy_without_pca = accuracy_score(y_test, y_pred)
accuracy_scores.append(accuracy_without_pca)
     
print("Logistic Regression without PCA")
print(f"Accuracy: {accuracy_without_pca:.4f}")
print(classification_report(y_test, y_pred))

pca_all = PCA()
X_train_pca_all = pca_all.fit_transform(X_train)
X_test_pca_all = pca_all.transform(X_test)
     
log_reg_pca_all = LogisticRegression()
log_reg_pca_all.fit(X_train_pca_all, y_train)
y_pred_pca_all = log_reg_pca_all.predict(X_test_pca_all)
     
accuracy_pca_all = accuracy_score(y_test, y_pred_pca_all)
accuracy_scores.append(accuracy_pca_all)
     
print("\nLogistic Regression with PCA (whole dataset)")
print(f"Accuracy: {accuracy_pca_all:.4f}")
print(classification_report(y_test, y_pred_pca_all))

pca_50 = PCA(0.5)  # Retain components that explain 50% of the variance
X_train_pca_50 = pca_50.fit_transform(X_train)
X_test_pca_50 = pca_50.transform(X_test)     

log_reg_pca_50 = LogisticRegression()
log_reg_pca_50.fit(X_train_pca_50, y_train)
y_pred_pca_50 = log_reg_pca_50.predict(X_test_pca_50)     

accuracy_pca_50 = accuracy_score(y_test, y_pred_pca_50)
accuracy_scores.append(accuracy_pca_50)     

print("\nLogistic Regression with PCA (variance explained ≥ 0.5)")
print(f"Accuracy: {accuracy_pca_50:.4f}")
print(classification_report(y_test, y_pred_pca_50))

pca_75 = PCA(0.75)  # Retain components that explain 75% of the variance
X_train_pca_75 = pca_75.fit_transform(X_train)
X_test_pca_75 = pca_75.transform(X_test)
     
log_reg_pca_75 = LogisticRegression()
log_reg_pca_75.fit(X_train_pca_75, y_train)
y_pred_pca_75 = log_reg_pca_75.predict(X_test_pca_75)     

accuracy_pca_75 = accuracy_score(y_test, y_pred_pca_75)
accuracy_scores.append(accuracy_pca_75)     

print("\nLogistic Regression with PCA (variance explained ≥ 0.75)")
print(f"Accuracy: {accuracy_pca_75:.4f}")
print(classification_report(y_test, y_pred_pca_75))

# Plotting Explained Variance for Each Scenario
plt.figure(figsize=(10, 6))
explained_variance = np.cumsum(pca_all.explained_variance_ratio_)
plt.plot(explained_variance, marker='o', linestyle='--', label='Cumulative Explained Variance (PCA All)')
plt.axhline(y=0.5, color='r', linestyle='--', label='0.5 Variance Threshold')
plt.axhline(y=0.75, color='g', linestyle='--', label='0.75 Variance Threshold')
plt.title('Explained Variance by Number of PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# Accuracy Comparison Graph
plt.figure(figsize=(8, 5))
scenarios = ['Without PCA', 'PCA All Components', 'PCA ≥ 0.5', 'PCA ≥ 0.75']
plt.bar(scenarios, accuracy_scores, color=['blue', 'green', 'orange', 'red'])
plt.title('Model Accuracy Across Different PCA Scenarios')
plt.xlabel('Scenario')
plt.ylabel('Accuracy')
plt.ylim(0.7, 1.0)
for i, v in enumerate(accuracy_scores):
    plt.text(i, v + 0.005, f"{v:.4f}", ha='center', fontsize=12)
plt.show()

# Confusion Matrix for the Best Model (You can choose the best model here)
best_model_conf_matrix = confusion_matrix(y_test, y_pred_pca_75)
plt.figure(figsize=(6, 4))
sns.heatmap(best_model_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for Best Model (PCA ≥ 0.75)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()